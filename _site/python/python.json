[
  {
    "path": "python/2021-11-02-random-forest/",
    "title": "Exploring hyper-parameter of Random forest",
    "description": "In this blog post I have explore the n_estimators and Max_features hpyer_parameter to find which value are the best for the model training and showed how can we select best values.",
    "author": [
      {
        "name": "Jaykumar patel",
        "url": {}
      }
    ],
    "date": "2021-06-11",
    "categories": [],
    "contents": "\r\nCreating and evaluating a simple random forest model\r\nIn this blog post I have created random forest and showed how we can evaluate the model.\r\nImporting Python Library\r\n\r\nimport pandas as pd\r\n\r\nImporting R library\r\n\r\n\r\nlibrary(reticulate)\r\nlibrary(tidyverse)\r\nlibrary(here)\r\nlibrary(ggthemes)\r\n\r\n\r\n\r\nRead the Data\r\n\r\n\r\ndf <- read.csv(file = here(\"_python/2021-11-02-random-forest\", \"house_data.csv\"))\r\nhead(df)\r\n\r\n\r\n  bedrooms bathrooms m2_living floors m2_above m2_basement\r\n1        3      1.50       124    1.5      124           0\r\n2        5      2.50       339    2.0      313          26\r\n3        3      2.00       179    1.0      179           0\r\n4        3      2.25       186    1.0       93          93\r\n5        4      2.50       180    1.0      106          74\r\n6        2      1.00        82    1.0       82           0\r\n  m2_lot view quality yr_built renovated_last_5 city\r\n1    735    0       3     1961                0   37\r\n2    841    4       5     1927                1   36\r\n3   1110    0       4     1972                1   19\r\n4    746    0       4     1969                1    4\r\n5    975    0       4     1982                0   32\r\n6    593    0       3     1944                0   36\r\n  statezip   price\r\n1       63  313000\r\n2       59 2384000\r\n3       27  342000\r\n4        8  420000\r\n5       32  550000\r\n6       55  490000\r\n\r\nCheck Null Value\r\n\r\nr.df.isnull().sum()\r\nbedrooms            0\r\nbathrooms           0\r\nm2_living           0\r\nfloors              0\r\nm2_above            0\r\nm2_basement         0\r\nm2_lot              0\r\nview                0\r\nquality             0\r\nyr_built            0\r\nrenovated_last_5    0\r\ncity                0\r\nstatezip            0\r\nprice               0\r\ndtype: int64\r\n\r\nSeperate response variables and explanatory variable\r\n\r\ny = r.df['price']\r\nX = r.df.drop('price', axis = 1)\r\n\r\nTrain and test split\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=0)\r\n \r\nprint('Shape of X_train = ', X_train.shape)\r\nShape of X_train =  (3680, 13)\r\nprint('Shape of X_validation = ', y_train.shape)\r\nShape of X_validation =  (3680,)\r\nprint('Shape of X_test = ', X_validation.shape)\r\nShape of X_test =  (920, 13)\r\nprint('Shape of y_validation = ',y_validation.shape)\r\nShape of y_validation =  (920,)\r\n\r\nCreat a random forest model\r\n\r\nfrom sklearn.ensemble import RandomForestRegressor\r\n \r\nregressor1 = RandomForestRegressor(random_state= 0)\r\nregressor1.fit(X_train, y_train)\r\nRandomForestRegressor(random_state=0)\r\n\r\nModel evalution\r\n\r\ntrain_pred = regressor1.predict(X_train)\r\nvalid_pred = regressor1.predict(X_validation)\r\n\r\nfrom sklearn.metrics import mean_absolute_error\r\n\r\ntrain_mae = mean_absolute_error(y_train, train_pred)\r\nvalid_mae = mean_absolute_error(y_validation, valid_pred)\r\n                                \r\nprint(f'Validation set mean absolute error is {round(valid_mae,2)}')\r\nValidation set mean absolute error is 142365.36\r\n\r\nExploring the n_estimators hyper-parameter\r\nIn this part\r\nI have used for loop to create a random forest model for each value of n_estimators from 1 to 30 to check for which estimator we are getting best performance.\r\n\r\nModel training\r\n\r\ntrain_mae2 = [] #stores MAE for training set for each n-estimators\r\nvalid_mae2 = [] #stores MAE for validation set for each n-estimators\r\n\r\nfor nesti in range(1,31):\r\n    regressor2 = RandomForestRegressor(n_estimators=nesti, random_state= 0)\r\n    regressor2.fit(X_train, y_train)\r\n    train_pred2 = regressor2.predict(X_train)\r\n    valid_pred2 = regressor2.predict(X_validation)\r\n    train_mae2.append(mean_absolute_error(y_train, train_pred2))\r\n    valid_mae2.append(mean_absolute_error(y_validation, valid_pred2))\r\nRandomForestRegressor(n_estimators=1, random_state=0)\r\nRandomForestRegressor(n_estimators=2, random_state=0)\r\nRandomForestRegressor(n_estimators=3, random_state=0)\r\nRandomForestRegressor(n_estimators=4, random_state=0)\r\nRandomForestRegressor(n_estimators=5, random_state=0)\r\nRandomForestRegressor(n_estimators=6, random_state=0)\r\nRandomForestRegressor(n_estimators=7, random_state=0)\r\nRandomForestRegressor(n_estimators=8, random_state=0)\r\nRandomForestRegressor(n_estimators=9, random_state=0)\r\nRandomForestRegressor(n_estimators=10, random_state=0)\r\nRandomForestRegressor(n_estimators=11, random_state=0)\r\nRandomForestRegressor(n_estimators=12, random_state=0)\r\nRandomForestRegressor(n_estimators=13, random_state=0)\r\nRandomForestRegressor(n_estimators=14, random_state=0)\r\nRandomForestRegressor(n_estimators=15, random_state=0)\r\nRandomForestRegressor(n_estimators=16, random_state=0)\r\nRandomForestRegressor(n_estimators=17, random_state=0)\r\nRandomForestRegressor(n_estimators=18, random_state=0)\r\nRandomForestRegressor(n_estimators=19, random_state=0)\r\nRandomForestRegressor(n_estimators=20, random_state=0)\r\nRandomForestRegressor(n_estimators=21, random_state=0)\r\nRandomForestRegressor(n_estimators=22, random_state=0)\r\nRandomForestRegressor(n_estimators=23, random_state=0)\r\nRandomForestRegressor(n_estimators=24, random_state=0)\r\nRandomForestRegressor(n_estimators=25, random_state=0)\r\nRandomForestRegressor(n_estimators=26, random_state=0)\r\nRandomForestRegressor(n_estimators=27, random_state=0)\r\nRandomForestRegressor(n_estimators=28, random_state=0)\r\nRandomForestRegressor(n_estimators=29, random_state=0)\r\nRandomForestRegressor(n_estimators=30, random_state=0)\r\n\r\nPlotting Model Performance graph\r\nPerformance on training set\r\n\r\n\r\nToDataframe <- function(data) {\r\n  new_df <- data.frame(matrix(unlist(data), nrow=length(data), byrow=TRUE)) # Converting python list to the dataframe\r\n  names(new_df)[1] <- \"error\"\r\nnew_df$enu <- seq.int(nrow(new_df)) # adding new column with the number of n_estimators \r\nreturn(new_df)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntrain_mae <- ToDataframe(py$train_mae2)\r\ntrain_mae %>%\r\n  ggplot(aes(x=enu, y=error))+\r\n  geom_line() +\r\n  labs(title = \"Distribution of MAE over n_estimators for training set\",\r\n         x = \"n_estimators\",\r\n         y = \"mean absolute error\") +\r\n    theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\nPerformance on validation set\r\n\r\n\r\nvalid_mae <- ToDataframe(py$valid_mae2)\r\n\r\nvalid_mae %>%\r\n  ggplot(aes(x=enu, y=error))+\r\n  geom_line() +\r\n  labs(title = \"Distribution of MAE over n_estimators for validation set\",\r\n         x = \"n_estimators\",\r\n         y = \"mean absolute error\") +\r\n    theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\nBest model performnace\r\n\r\nminimum2 = valid_mae2.index(min(valid_mae2))\r\nprint(f'Minimum Mean Absolute error is {round(min(valid_mae2),2)}\\nMinimum error is at {minimum2 + 1} n_estimators ')\r\nMinimum Mean Absolute error is 138483.76\r\nMinimum error is at 4 n_estimators \r\n\r\nOverall observation\r\nWhich value of n_estimators gives the best results for the validation set?\r\n4 n_estimators gives the best results\r\n\r\nHow I decided that this value for n_estimators gave the best results?\r\nI created model for n_estimators from 1 to 30 and stored value in the list valid_mae2.\r\nThen I compare all the results and looked for the minimum error that is how I decided the value of the n_estimators.\r\n\r\nExploring the max_features hyper-parameter\r\nIn this part\r\nI have used for loop to create a random forest model for each value of max_features from 1 to number of features present in the data.\r\nI have used n-estimatro 4 as we found that it gives us the best performance.\r\n\r\nModel training\r\n\r\n\r\ntrain_mae3 = [] #stores MAE for training set for each n-estimators\r\nvalid_mae3 = [] #stores MAE for training set for each n-estimators\r\n\r\nfor max_fea in range(1,14):\r\n    regressor3 = RandomForestRegressor(n_estimators=4, max_features=max_fea, random_state= 0) \r\n    regressor3.fit(X_train, y_train)\r\n    train_pred3 = regressor3.predict(X_train)\r\n    valid_pred3 = regressor3.predict(X_validation)\r\n    train_mae3.append(mean_absolute_error(y_train, train_pred3))\r\n    valid_mae3.append(mean_absolute_error(y_validation, valid_pred3))\r\nRandomForestRegressor(max_features=1, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=2, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=3, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=4, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=5, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=6, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=7, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=8, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=9, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=10, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=11, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=12, n_estimators=4, random_state=0)\r\nRandomForestRegressor(max_features=13, n_estimators=4, random_state=0)\r\n\r\nPlotting Model Performance graph\r\n\r\n\r\ntrain_mae <- ToDataframe(py$train_mae3)\r\ntrain_mae %>%\r\n  ggplot(aes(x=enu, y=error))+\r\n  geom_line() +\r\n  labs(title = \"Distribution of MAE over Max_features for training set\",\r\n         x = \"# of features\",\r\n         y = \"mean absolute error\") +\r\n    theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\n\r\n\r\nvalid_mae <- ToDataframe(py$valid_mae3)\r\nvalid_mae %>%\r\n  ggplot(aes(x=enu, y=error))+\r\n  geom_line() +\r\n  labs(title = \"Distribution of MAE over Max_features for validation set\",\r\n         x = \"# of features\",\r\n         y = \"mean absolute error\") +\r\n    theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\n## Best model performance\r\n\r\nminimum3 = valid_mae3.index(min(valid_mae3))\r\nprint(f'Minimum Mean Absolute error is {round(min(valid_mae3),2)}\\nMinimum error is at {minimum3 + 1} max_features ')\r\nMinimum Mean Absolute error is 138483.76\r\nMinimum error is at 13 max_features \r\n\r\n\r\n\r\n\r\n",
    "preview": "python/2021-11-02-random-forest/forest.gif",
    "last_modified": "2021-11-02T03:41:50-04:00",
    "input_file": "random-forest.knit.md"
  },
  {
    "path": "python/2021-05-06-monthly-provisional-counts-of-deaths/",
    "title": "Monthly Provisional Counts Of Deaths",
    "description": "In this blog I have used Python packages in the Rstudio and done the EDA as well as Summary Statistics on the cause of the death in the united state of america in the year 2019 and 2020.",
    "author": [
      {
        "name": "Jaykumar patel",
        "url": {}
      }
    ],
    "date": "2021-05-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAnalysis of the Monthly Provisional Death counts\r\nImporting Required Libraries\r\nReading Data\r\nSummary Statistics for Numerical Columns\r\nAllCause\r\nNaturalCause\r\nSepticemia (A40-A41)\r\nMalignant neoplasms (C00-C97)\r\nFor the remaining columns\r\n\r\nSummary Statistics for Categorical Data\r\nRace/Ethnicity\r\nSex\r\nAgeGroup\r\n\r\nExploratory Data Analysis Numeric Data\r\nPlotting histogram for AllCause Column\r\nRemoving Skewness\r\n\r\nChecking for the outliers for All Cause column\r\nOutliears after applying Log\r\n\r\n\r\nConclusion of numerical data exploration\r\nExploratory Data Analysis chategorical Data\r\nPlotting Bar Graph for AgeGroup column\r\n\r\n\r\nAnalysis of the Monthly Provisional Death counts\r\nIn this analysis we going to look into the cause of death in the United State in the year of 2019 and 2020.\r\nImporting Required Libraries\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nReading Data\r\n\r\ndeath = pd.read_csv('Data/causes_of_death.csv', index_col= 0)\r\n\r\n\r\ndeath.head()\r\n   Date.Of.Death.Year  ...  Jurisdiction.of.Occurrence\r\n1                2019  ...               United States\r\n2                2019  ...               United States\r\n3                2019  ...               United States\r\n4                2019  ...               United States\r\n5                2019  ...               United States\r\n\r\n[5 rows x 23 columns]\r\n\r\nSummary Statistics for Numerical Columns\r\nThe describe() function computes a summary of statistics pertaining to the DataFrame columns.\r\nDescribe function gives the mean, std and IQR values.\r\nAllCause\r\n\r\ndeath['AllCause'].describe()\r\ncount     2849.000000\r\nmean      2280.159354\r\nstd       6028.864306\r\nmin         10.000000\r\n25%         86.000000\r\n50%        271.000000\r\n75%       1498.000000\r\nmax      53242.000000\r\nName: AllCause, dtype: float64\r\n\r\nThis column tells us about total number of people died in the month of particular year in the United State of America.\r\nFrom the count row we can find that we have 2849 non null values in our data and remaining rows contain null values.\r\nIn this column we can see that the mean value is 2280.159354 and median (50%) is 271, that means our data is right skewed distributed.\r\nSo the frequency of people died at very large number is not high we can say that there are some outliears which leads to this difference.\r\n\r\nStandard deviation of this column is 6028.864306. It indicates data are more spread out. A high standard deviation means data is not closely bound to the mean value.\r\nData is not containing close Continuous number for the people died in the United State of America. It has very large range.\r\nAs we can see the range is 0-53242 and we have only 2849 data, This is why we have high standard deviation and large difference between mean and median.\r\nNaturalCause\r\n\r\ndeath['NaturalCause'].describe()\r\ncount     2717.000000\r\nmean      2194.944056\r\nstd       5945.745418\r\nmin          0.000000\r\n25%         70.000000\r\n50%        216.000000\r\n75%       1294.000000\r\nmax      52054.000000\r\nName: NaturalCause, dtype: float64\r\n\r\nThis column tells us about total number of people died in the month of particular year in the United State of America due to natural cause.\r\nFrom the count row we can find that we have 2717 non null values in our data and remaining rows contain null values.\r\nIn this column we can see that the mean value is 2194.944056 and median (50%) is 216, that means our data is right skewed distributed.\r\nWe have the closest mean and median to the all cause’s mean and median. It means the both are having the some what similar numbers for observation.\r\n\r\nStandard deviation of this column is 5945.745418. It indicates data are more spread out. A high standard deviation means data is not closely bound to the mean value.\r\nIt has very close stadard deviation as allcause column which also add support to the statement that they have some identical numbers or nearest numbers\r\n\r\nSepticemia (A40-A41)\r\n\r\ndeath['Septicemia..A40.A41.'].describe()\r\ncount    1736.000000\r\nmean       44.580069\r\nstd        88.269978\r\nmin         0.000000\r\n25%         0.000000\r\n50%        10.000000\r\n75%        36.000000\r\nmax       484.000000\r\nName: Septicemia..A40.A41., dtype: float64\r\n\r\nThis column tells us about number of people died in the month of particular year in the United State of America due to Septicemia (A40-A41) disease.\r\nFrom the count row we can find that we have 1736 non null values in our data and remaining rows contain null values.\r\nIn this column we can see that the mean value is 44.580069 and median (50%) is 10, that means our data is right skewed distributed.\r\nAround 44 people die every month from this disease.\r\n\r\nStandard deviation of this column is 88.269978. It has the lowest standard deviation of all.\r\nMalignant neoplasms (C00-C97)\r\n\r\ndeath['Malignant.neoplasms..C00.C97.'].describe()\r\ncount    2249.000000\r\nmean      549.167185\r\nstd      1277.215975\r\nmin         0.000000\r\n25%        20.000000\r\n50%        69.000000\r\n75%       338.000000\r\nmax      6498.000000\r\nName: Malignant.neoplasms..C00.C97., dtype: float64\r\n\r\nThis column tells us about total number of people died in the month of particular year in the United State of America due to Malignant neoplasms.\r\nFrom the count row we can find that we have 2249 non null values in our data and remaining rows contain null values.\r\nIn this column we can see that the mean value is 549.167185 and median (50%) is 69, that means our data is right skewed distributed.\r\nWe can say that around 550 people die every month. As compared to the Septicemia more than 12 times more people died due to this disease.\r\n\r\nStandard deviation of this column is 1277.215975. It indicates data are more spread out. A high standard deviation means data is not closely bound to the mean value.\r\nIt has not as large standard deviation as of allcause column but still it has a high standard deviation which means data is not close to it mean so we can not say that our statement for death for everymonth is accurate.\r\n\r\nFor the remaining columns\r\nWe can see that the every column have the low median value and the high mean value. Which means all the column are right skewed.\r\nWe can say that the data frame is not closely bound to its mean values. Our data have very distinct values and its difference from minmum to maximum values are very high.\r\nThis difference means there are very few deaths have high frequency and number of deaths for months are comparatively smallar than it mean values.\r\nHere, we can not state that the number of people died in the month is equal to the mean value and reason is that, as we can see we have very small median and at the same time we have high mean.\r\nThis means that most of the data is less than the mean value there are some outliers. Those outliers are the reason we are having a large mean value.\r\n\r\nSummary Statistics for Categorical Data\r\nRace/Ethnicity\r\nThis column tells us about total number of people died as per their race in the month of particular year in the United State of America.\r\n\r\nprint(death.groupby(['Race.Ethnicity']).size())\r\nRace.Ethnicity\r\nHispanic                                         500\r\nNon-Hispanic American Indian or Alaska Native    500\r\nNon-Hispanic Asian                               500\r\nNon-Hispanic Black                               500\r\nNon-Hispanic White                               500\r\nOther                                            500\r\ndtype: int64\r\n\r\nOur data have uniform distribution when we consider the races.\r\nIt is good to have uniform distribution it help use to train a model which is not bias.\r\nSex\r\nThis column tells us about total number of people died as per their sex in the month of particular year in the United State of America.\r\n\r\nprint(death.groupby(['Sex']).size())\r\nSex\r\nF         720\r\nFemale    780\r\nM         720\r\nMale      780\r\ndtype: int64\r\n\r\nHere also,\r\n* Our data have uniform distribution when we look for the sex. * Having uniform distribution in the data is help us to see how each category affects\r\nAgeGroup\r\nThis column tells us about total number of people died as per their agegroup in the month of particular year in the United State of America.\r\n\r\nprint (death.groupby(['AgeGroup']).size())\r\nAgeGroup\r\n0-4 years            300\r\n15-24 years          300\r\n25-34 years          300\r\n35-44 years          300\r\n45-54 years          300\r\n5-14 years           300\r\n55-64 years          300\r\n65-74 years          300\r\n75-84 years          300\r\n85 years and over    300\r\ndtype: int64\r\n\r\nSame as above data column it also have uniform distribution so if we want to train our model to predict the cause of the death.\r\nWe can use following columns:\r\nRace/Ethnicity\r\nSex\r\nAgeGroup\r\nExploratory Data Analysis Numeric Data\r\nPlotting histogram for AllCause Column\r\n\r\n\r\npy$death %>% ggplot(aes(AllCause))+    \r\n  geom_histogram(aes(y = stat(density)), color = \"#13B4FA\",fill = \"#FF6F91\", bins = 40) +\r\n    geom_density(fill = \"#845EC2\", alpha = 0.5, color = NaN)+\r\n  labs(title = \"Distribution of Total Death\",\r\n         x = \"Total Death\",\r\n         y = \"density\") +\r\n    theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\nWe can see here that our data have a right skewness so if we are going to use this data column for the modeling than we need to first remove the skewnees then we can do modeling.\r\nRemoving Skewness\r\n\r\n\r\np1 <- py$death %>%\r\n    ggplot(aes(AllCause)) +\r\n    geom_histogram(aes(y = stat(density)),bins = 28, color = \"#13B4FA\",fill = \"#FF6F91\") +\r\n    geom_density(fill = \"#845EC2\", alpha = 0.5, color = NaN)+\r\n    labs(title = \"Total Death\",\r\n         x = \"Total Death\",\r\n         y = \"density\") +\r\n    theme_minimal() + themes()\r\n\r\np2 <- py$death %>%\r\n    ggplot(aes(log10(AllCause))) +\r\n    geom_histogram(aes(y = stat(density)),bins = 28, color = \"#13B4FA\",fill = \"#FFC75F\") +\r\n    geom_density(fill = \"#845EC2\", alpha = 0.5, color = NaN)+\r\n    labs(title = \"Total Death (log10 based)\",\r\n         x = \"Total Death\",\r\n         y = \"density\") +\r\n    theme_minimal()+ themes()\r\n\r\np3 <- py$death %>%\r\n    ggplot(aes(sqrt(AllCause))) +\r\n    geom_histogram(aes(y = stat(density)),bins = 28, color = \"#13B4FA\",fill = \"#845EC2\") +\r\n    geom_density(fill = \"#845EC2\", alpha = 0.5, color = NaN)+\r\n    labs(title = \"Total Death(Sqrt. based)\",\r\n         x = \"Total Death\",\r\n         y = \"density\") +\r\n    theme_minimal() + themes()\r\n\r\ngrid.arrange(p1,p2,p3, ncol = 2)\r\n\r\n\r\n\r\n\r\nNow if we look at the distribution of the AllCause we get the normal distribution. By applying log10, we can convert skewness to normal distribution.\r\nChecking for the outliers for All Cause column\r\n\r\n\r\npy$death %>% \r\n  ggplot(aes(y = AllCause))+\r\n  geom_boxplot(outlier.colour = \"#651a34\")+\r\n  theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\nWe can clear see that there are a lot of outliers present in this column. There is a way to remove those that is by applying the Log.\r\nOutliears after applying Log\r\n\r\n\r\npy$death %>% \r\n  ggplot(aes(y = log10(AllCause)))+\r\n  geom_boxplot(outlier.colour = \"#651a34\")+\r\n  theme_minimal()+\r\n  themes()\r\n\r\n\r\n\r\n\r\nAs we can see now, there is no outliers present in the column so now, we can use this to train out model.\r\nConclusion of numerical data exploration\r\nWe have seen the method to remove skewnees and outliers from the data. We always keep this in mind that before moving to the modeling part we always need to do the exploration of the data, so we can know our data better.\r\nIts behavior, is there any pattern present in the data.\r\n\r\nExploratory Data Analysis chategorical Data\r\nIn this part we are going to plot some graph to check skewness of the chategorical data and outliers present in that data.\r\nUnlike numerical data we can not use histogram to find out skewness, we need to plot bar graph.\r\nPlotting Bar Graph for AgeGroup column\r\n\r\n\r\nAge <- py$death %>% \r\n          select(AgeGroup) %>% \r\n          group_by(AgeGroup) %>% \r\n          summarize(frequency = n())\r\n\r\nAge %>% \r\n  ggplot(aes(AgeGroup,frequency))+\r\n  geom_col(stat = \"identity\", fill = \"#651a34\" )+\r\n  coord_flip()+\r\n  theme_minimal()+\r\n  labs( title = \"Distribution of Age Group\"\r\n  )+\r\n  themes()\r\n\r\n\r\n\r\n\r\nWe can see here that our data is evenly distributed in the age group. Every age group have same number of death. So there is no need to do any transformation.\r\n\r\n\r\n\r\n",
    "preview": "https://media.giphy.com/media/3orif8V09vBtyRFVU4/giphy.gif",
    "last_modified": "2021-10-30T16:16:17-04:00",
    "input_file": {}
  }
]
